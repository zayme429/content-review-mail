#!/usr/bin/env python3
"""
å†…å®¹è‡ªåŠ¨åŒ–Pipeline - å®Œæ•´ç‰ˆ
æ”¯æŒï¼šRSSé‡‡é›† â†’ AIé€‰é¢˜ â†’ AIæ’°å†™ â†’ å®¡æ ¸é€šçŸ¥ â†’ è‡ªåŠ¨/æ‰‹åŠ¨å‘å¸ƒ â†’ å®šæ—¶ä»»åŠ¡
"""

import os
import sys
import json
import logging
import requests
import subprocess
from datetime import datetime
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from fetcher.rss_collector import RSSCollector
from generator.content_generator import ContentGenerator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/.openclaw/workspace/content-pipeline/logs/pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ContentPipeline:
    def __init__(self):
        self.base_dir = Path('/root/.openclaw/workspace/content-pipeline')
        self.config = self._load_config()
        self.memory = self._load_memory()
        self.collector = RSSCollector()
        self.generator = ContentGenerator()
        
        # é£ä¹¦/ä¼å¾®é€šçŸ¥é…ç½®
        self.notify_channel = 'feishu'  # æˆ– 'wecom-app'
        
    def _load_config(self):
        config_path = self.base_dir / 'config' / 'pipeline.json'
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_memory(self):
        memory_path = self.base_dir / 'memory' / 'published.json'
        if memory_path.exists():
            with open(memory_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {'articles': [], 'topics': [], 'pending_review': []}
    
    def _save_memory(self):
        memory_path = self.base_dir / 'memory' / 'published.json'
        with open(memory_path, 'w', encoding='utf-8') as f:
            json.dump(self.memory, f, ensure_ascii=False, indent=2)
    
    def notify_user(self, title, content, actions=None):
        """é€šçŸ¥ç”¨æˆ·å®¡æ ¸"""
        logger.info(f"ğŸ“¢ å‘é€é€šçŸ¥: {title}")
        
        # æ„å»ºé€šçŸ¥æ¶ˆæ¯
        message = f"""ğŸ“ **æ–‡ç« å¾…å®¡æ ¸**

**æ ‡é¢˜**: {title}

**æ‘˜è¦**:
{content[:500]}...

**æ“ä½œ**:
1. æŸ¥çœ‹å®Œæ•´æ–‡ç« : `/content-pipeline/output/article_*.md`
2. ç¡®è®¤å‘å¸ƒ: å›å¤ "å‘å¸ƒ"
3. é‡æ–°ç”Ÿæˆ: å›å¤ "é‡æ–°ç”Ÿæˆ [æ–°çš„é€‰é¢˜æ–¹å‘]"
4. è·³è¿‡ä»Šæ—¥: å›å¤ "è·³è¿‡"

**æˆªæ­¢æ—¶é—´**: 30åˆ†é’Ÿå†…æœªå›å¤å°†è‡ªåŠ¨å‘å¸ƒåˆ°è‰ç¨¿ç®±
"""
        
        # è¿™é‡Œå¯ä»¥é€šè¿‡é£ä¹¦/ä¼å¾®APIå‘é€
        # ç›®å‰å…ˆè¾“å‡ºåˆ°æ—¥å¿—
        logger.info(f"\n{'='*60}")
        logger.info(message)
        logger.info(f"{'='*60}\n")
        
        return True
    
    def step1_collect(self):
        """æ­¥éª¤1: é‡‡é›†çƒ­ç‚¹èµ„è®¯"""
        logger.info("=== æ­¥éª¤1: é‡‡é›†çƒ­ç‚¹èµ„è®¯ ===")
        
        items = self.collector.collect_all()
        if not items:
            logger.warning("æœªé‡‡é›†åˆ°æœ‰æ•ˆèµ„è®¯")
            return []
        
        scored_items = self.collector.score_items(items)
        top_items = scored_items[:10]
        logger.info(f"âœ… é‡‡é›†å®Œæˆï¼Œç²¾é€‰ {len(top_items)} æ¡")
        
        return top_items
    
    def step2_analyze(self, collected_items):
        """æ­¥éª¤2: AIåˆ†æé€‰é¢˜è§’åº¦"""
        logger.info("=== æ­¥éª¤2: AIåˆ†æé€‰é¢˜è§’åº¦ ===")
        
        recent_topics = [a.get('topic', '') for a in self.memory.get('articles', [])[-20:]]
        analysis = self.generator.analyze_topic(collected_items, recent_topics)
        
        logger.info(f"âœ… é€‰å®šé€‰é¢˜: {analysis.get('title', 'AIæ—¶ä»£çš„å­¦ä¹ ä¸æˆé•¿')}")
        return analysis
    
    def step3_write(self, angle_info):
        """æ­¥éª¤3: æ’°å†™æ–‡ç« """
        logger.info("=== æ­¥éª¤3: AIæ’°å†™æ–‡ç«  ===")
        
        article = self.generator.write_article(angle_info)
        
        # æ¸…ç†markdownä»£ç å—æ ‡è®°
        article = article.replace('```markdown\n', '').replace('\n```', '')
        
        return article
    
    def step4_save(self, article, topic):
        """æ­¥éª¤4: ä¿å­˜æ–‡ç« å¹¶ç­‰å¾…å®¡æ ¸"""
        logger.info("=== æ­¥éª¤4: ä¿å­˜å¹¶é€šçŸ¥å®¡æ ¸ ===")
        
        today = datetime.now().strftime('%Y%m%d')
        output_path = self.base_dir / 'output' / f'article_{today}.md'
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(article)
        
        logger.info(f"âœ… æ–‡ç« å·²ä¿å­˜: {output_path}")
        
        # æ·»åŠ åˆ°å¾…å®¡æ ¸åˆ—è¡¨
        review_item = {
            'date': datetime.now().isoformat(),
            'topic': topic,
            'path': str(output_path),
            'status': 'pending_review'
        }
        self.memory['pending_review'].append(review_item)
        self._save_memory()
        
        # å‘é€é€šçŸ¥
        self.notify_user(topic, article)
        
        return output_path
    
    def step5_publish(self, article_path, auto_publish=False):
        """æ­¥éª¤5: å‘å¸ƒåˆ°å¾®ä¿¡å…¬ä¼—å·è‰ç¨¿ç®±"""
        logger.info("=== æ­¥éª¤5: å‘å¸ƒåˆ°è‰ç¨¿ç®± ===")
        
        if not auto_publish:
            logger.info("â³ ç­‰å¾…å®¡æ ¸ä¸­... (è®¾ç½® auto_publish=True å¯è‡ªåŠ¨å‘å¸ƒ)")
            return None
        
        try:
            # ä½¿ç”¨wenyan-cliå‘å¸ƒ
            result = subprocess.run(
                ['wenyan', 'publish', '-f', str(article_path), '-t', 'lapis', '-h', 'solarized-light'],
                capture_output=True,
                text=True,
                timeout=120,
                env={**os.environ, 'WECHAT_APP_ID': 'wx5c6f2e9b5734ddd5', 'WECHAT_APP_SECRET': 'baf071b9ca8e805992a26111c552b9f9'}
            )
            
            if 'ä¸Šä¼ æˆåŠŸ' in result.stdout or 'media_id' in result.stdout:
                logger.info("âœ… å‘å¸ƒæˆåŠŸ")
                return result.stdout
            else:
                logger.warning(f"âš ï¸ å‘å¸ƒè¿”å›: {result.stdout} {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ å‘å¸ƒå¤±è´¥: {e}")
            return None
    
    def run(self, auto_publish=False, skip_review=False):
        """æ‰§è¡Œå®Œæ•´Pipeline"""
        logger.info("ğŸš€ å¯åŠ¨å†…å®¹è‡ªåŠ¨åŒ–Pipeline")
        
        try:
            # æ­¥éª¤1: é‡‡é›†
            collected = self.step1_collect()
            if not collected:
                logger.warning("æœªé‡‡é›†åˆ°æœ‰æ•ˆèµ„è®¯ï¼ŒPipelineç»ˆæ­¢")
                return False
            
            # æ­¥éª¤2: åˆ†æ
            angle = self.step2_analyze(collected)
            topic = angle.get('title', 'AIæ—¶ä»£çš„å­¦ä¹ ä¸æˆé•¿')
            
            # æ­¥éª¤3: æ’°å†™
            article = self.step3_write(angle)
            
            # æ­¥éª¤4: ä¿å­˜å¹¶é€šçŸ¥å®¡æ ¸
            article_path = self.step4_save(article, topic)
            
            # æ­¥éª¤5: å‘å¸ƒï¼ˆå¦‚æœè®¾ç½®è‡ªåŠ¨å‘å¸ƒæˆ–è·³è¿‡å®¡æ ¸ï¼‰
            if auto_publish or skip_review:
                self.step5_publish(article_path, auto_publish=True)
                status = 'published'
            else:
                logger.info("â³ æ–‡ç« å·²ä¿å­˜ï¼Œç­‰å¾…äººå·¥å®¡æ ¸åå‘å¸ƒ")
                logger.info(f"   æ–‡ä»¶: {article_path}")
                status = 'pending_review'
            
            # æ›´æ–°è®°å¿†
            self.memory['articles'].append({
                'date': datetime.now().isoformat(),
                'topic': topic,
                'path': str(article_path),
                'status': status
            })
            self._save_memory()
            
            logger.info("âœ… Pipelineæ‰§è¡Œå®Œæˆï¼")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Pipelineæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return False
    
    def review_and_publish(self, article_date=None, action='publish'):
        """å®¡æ ¸å¹¶å‘å¸ƒæŒ‡å®šæ–‡ç« """
        if article_date is None:
            article_date = datetime.now().strftime('%Y%m%d')
        
        article_path = self.base_dir / 'output' / f'article_{article_date}.md'
        
        if not article_path.exists():
            logger.error(f"âŒ æ–‡ç« ä¸å­˜åœ¨: {article_path}")
            return False
        
        if action == 'publish':
            result = self.step5_publish(article_path, auto_publish=True)
            if result:
                logger.info("âœ… æ–‡ç« å·²å‘å¸ƒåˆ°å¾®ä¿¡å…¬ä¼—å·è‰ç¨¿ç®±")
                return True
        elif action == 'skip':
            logger.info("â­ï¸ è·³è¿‡å‘å¸ƒ")
            return True
        
        return False

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Content Pipeline')
    parser.add_argument('--auto-publish', action='store_true', help='è‡ªåŠ¨å‘å¸ƒï¼ˆè·³è¿‡å®¡æ ¸ï¼‰')
    parser.add_argument('--review', type=str, help='å®¡æ ¸å¹¶å‘å¸ƒæŒ‡å®šæ—¥æœŸæ–‡ç«  (YYYYMMDD)')
    parser.add_argument('--action', type=str, default='publish', choices=['publish', 'skip'], help='å®¡æ ¸åŠ¨ä½œ')
    
    args = parser.parse_args()
    
    pipeline = ContentPipeline()
    
    if args.review:
        # å®¡æ ¸æ¨¡å¼
        success = pipeline.review_and_publish(args.review, args.action)
        sys.exit(0 if success else 1)
    else:
        # æ­£å¸¸è¿è¡Œ
        success = pipeline.run(auto_publish=args.auto_publish)
        sys.exit(0 if success else 1)
