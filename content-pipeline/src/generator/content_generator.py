#!/usr/bin/env python3
"""
LLMå†…å®¹ç”Ÿæˆå™¨ - è°ƒç”¨Kimiç”Ÿæˆæ–‡ç« 
"""

import os
import json
import requests
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class ContentGenerator:
    def __init__(self):
        # ä»OpenClawé…ç½®è¯»å–API key
        self.api_key = self._load_api_key()
        self.base_url = "https://api.moonshot.cn/v1"
        self.model = "kimi-k2.5"
        
    def _load_api_key(self):
        """ä»OpenClawé…ç½®è¯»å–API key"""
        try:
            config_path = Path.home() / '.openclaw' / 'openclaw.json'
            with open(config_path, 'r') as f:
                config = json.load(f)
            return config['models']['providers']['moonshot']['apiKey']
        except:
            # å°è¯•ç¯å¢ƒå˜é‡
            return os.environ.get('MOONSHOT_API_KEY')
    
    def _call_llm(self, prompt, temperature=0.7):
        """è°ƒç”¨LLM API"""
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.model,
                'messages': [
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç§‘æŠ€ä¸“æ ä½œå®¶ï¼Œä¸“æ³¨äºAIæ—¶ä»£çš„ä¸ªäººæˆé•¿ä¸èŒä¸šå‘å±•ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 1,  # kimi-k2.5 åªæ”¯æŒ temperature=1
                'max_tokens': 4000
            }
            
            logger.info("ğŸ¤– è°ƒç”¨LLMç”Ÿæˆå†…å®¹...")
            response = requests.post(
                f'{self.base_url}/chat/completions',
                headers=headers,
                json=data,
                timeout=300
            )
            
            result = response.json()
            
            # æ£€æŸ¥é”™è¯¯
            if 'error' in result:
                logger.error(f"âŒ APIè¿”å›é”™è¯¯: {result['error']}")
                raise Exception(f"API Error: {result['error']}")
            
            if 'choices' not in result:
                logger.error(f"âŒ  unexpected response: {json.dumps(result, ensure_ascii=False)[:500]}")
                raise KeyError("'choices' not in response")
            
            content = result['choices'][0]['message']['content']
            logger.info("âœ… LLMç”Ÿæˆå®Œæˆ")
            return content
            
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def analyze_topic(self, news_items, recent_topics):
        """åˆ†æé€‰é¢˜è§’åº¦"""
        logger.info("=== AIåˆ†æé€‰é¢˜ ===")
        
        # åŠ è½½ç®€åŒ–æç¤ºè¯
        prompt_path = Path('/root/.openclaw/workspace/content-pipeline/config/prompts/analyze_topic_simple.md')
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt_template = f.read()
        
        # å‡†å¤‡æ–°é—»å†…å®¹
        news_text = ""
        for i, item in enumerate(news_items[:5], 1):
            news_text += f"\n{i}. {item['title']}\n   æ¥æº: {item['source']}\n"
        
        # å‡†å¤‡å†å²ä¸»é¢˜
        topics_text = ", ".join(recent_topics[-5:]) if recent_topics else "æ— "
        
        # æ›¿æ¢æ¨¡æ¿å˜é‡
        prompt = prompt_template.replace('{news_items}', news_text).replace('{recent_topics}', topics_text)
        
        # è°ƒç”¨LLM
        response = self._call_llm(prompt, temperature=0.8)
        
        # è§£ææ–‡æœ¬ç»“æœ
        try:
            lines = response.strip().split('\n')
            result = {'title': '', 'angle': '', 'target': '', 'value': ''}
            
            for line in lines:
                if 'é€‰é¢˜æ ‡é¢˜ï¼š' in line or 'é€‰é¢˜æ ‡é¢˜:' in line:
                    result['title'] = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                elif 'æ ¸å¿ƒè§’åº¦ï¼š' in line or 'æ ¸å¿ƒè§’åº¦:' in line:
                    result['angle'] = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                elif 'ç›®æ ‡è¯»è€…ï¼š' in line or 'ç›®æ ‡è¯»è€…:' in line:
                    result['target'] = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                elif 'ä»·å€¼ç‚¹ï¼š' in line or 'ä»·å€¼ç‚¹:' in line:
                    result['value'] = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
            
            # å¦‚æœæ²¡è§£æåˆ°æ ‡é¢˜ï¼Œç”¨ç¬¬ä¸€è¡Œ
            if not result['title'] and lines:
                result['title'] = lines[0][:50]
            
            logger.info(f"âœ… é€‰é¢˜åˆ†æå®Œæˆ: {result['title'][:40]}...")
            return result
            
        except Exception as e:
            logger.error(f"âŒ è§£æåˆ†æç»“æœå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return {'title': 'AIæ—¶ä»£çš„å­¦ä¹ ä¸æˆé•¿', 'angle': 'ä»è¢«åŠ¨æ¥å—åˆ°ä¸»åŠ¨å­¦ä¹ ', 'target': 'èŒåœºäººå£«', 'value': 'æŒæ¡AIæ—¶ä»£çš„å­¦ä¹ æ–¹æ³•'}
    
    def write_article(self, topic_info):
        """æ’°å†™æ–‡ç« """
        logger.info("=== AIæ’°å†™æ–‡ç«  ===")
        
        # åŠ è½½ç®€åŒ–æç¤ºè¯
        prompt_path = Path('/root/.openclaw/workspace/content-pipeline/config/prompts/write_article_simple.md')
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt_template = f.read()
        
        # æ›¿æ¢æ¨¡æ¿å˜é‡
        prompt = (prompt_template
            .replace('{article_title}', topic_info.get('title', 'AIæ—¶ä»£çš„å­¦ä¹ ä¸æˆé•¿'))
            .replace('{core_angle}', topic_info.get('angle', topic_info.get('value', 'æ·±å…¥åˆ†æAIæ—¶ä»£çš„å˜åŒ–')))
            .replace('{target_audience}', topic_info.get('target', 'èŒåœºäººå£«')))
        
        # è°ƒç”¨LLM
        article = self._call_llm(prompt, temperature=0.7)
        
        logger.info(f"âœ… æ–‡ç« æ’°å†™å®Œæˆï¼Œé•¿åº¦: {len(article)} å­—ç¬¦")
        return article

if __name__ == '__main__':
    # æµ‹è¯•
    gen = ContentGenerator()
    
    # æµ‹è¯•é€‰é¢˜åˆ†æ
    test_news = [
        {'title': 'ChatGPTå‘å¸ƒæ–°åŠŸèƒ½', 'summary': 'OpenAIå‘å¸ƒ...', 'source': 'æœºå™¨ä¹‹å¿ƒ'}
    ]
    analysis = gen.analyze_topic(test_news, [])
    print(json.dumps(analysis, ensure_ascii=False, indent=2))
